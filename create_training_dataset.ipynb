{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "Kopie van create-training-dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj2oWc2p5sCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a305856-514f-435b-f53b-77683c410b26"
      },
      "source": [
        "!pip install -q -U keras-tuner\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10kB 20.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 26.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 29.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 31.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 29.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 31.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 8.5MB/s \n",
            "\u001b[?25h  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSSkhkyY_J1X"
      },
      "source": [
        "# Imports\n",
        "import gdown\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import csv   \n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import scipy.misc\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import backend as K\n",
        "import tensorflow.keras.layers.experimental.preprocessing as keras_preproc\n",
        "import kerastuner as kt\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyDe7oML_J1e",
        "outputId": "786a6bf1-ffb7-4b63-d960-95850caaf2d2"
      },
      "source": [
        "# Download provided dataset\n",
        "!wget files.brainfriz.com/train_images.zip # secondary link for images\n",
        "!unzip -qq -o train_images.zip\n",
        "!rm train_images.zip\n",
        "!gdown --id \"1xbEVK_NigW_5ngwKMHvuOTehYhT2v2WF\" # labels\n",
        "!gdown --id \"1SvI9dN2_25c2OlevwK4TjmzBNysjE_PO\" # label mapping"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-24 06:42:38--  http://files.brainfriz.com/train_images.zip\n",
            "Resolving files.brainfriz.com (files.brainfriz.com)... 138.201.201.196\n",
            "Connecting to files.brainfriz.com (files.brainfriz.com)|138.201.201.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.brainfriz.com/train_images.zip [following]\n",
            "--2021-03-24 06:42:38--  https://files.brainfriz.com/train_images.zip\n",
            "Connecting to files.brainfriz.com (files.brainfriz.com)|138.201.201.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2569658627 (2.4G) [application/zip]\n",
            "Saving to: ‘train_images.zip’\n",
            "\n",
            "train_images.zip    100%[===================>]   2.39G  30.8MB/s    in 80s     \n",
            "\n",
            "2021-03-24 06:43:58 (30.8 MB/s) - ‘train_images.zip’ saved [2569658627/2569658627]\n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xbEVK_NigW_5ngwKMHvuOTehYhT2v2WF\n",
            "To: /content/train.csv\n",
            "100% 358k/358k [00:00<00:00, 5.72MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SvI9dN2_25c2OlevwK4TjmzBNysjE_PO\n",
            "To: /content/label_num_to_disease_map.json\n",
            "100% 172/172 [00:00<00:00, 153kB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRv38K3LHvGQ"
      },
      "source": [
        "IMG_SIZE = 512\n",
        "\n",
        "def augment_image(img):\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
        "  img4d = tf.expand_dims(img, 0)\n",
        "  data_augmentation = tf.keras.Sequential([\n",
        "    keras_preproc.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "    keras_preproc.RandomRotation(0.2),\n",
        "    keras_preproc.RandomZoom((0,-0.3)),\n",
        "  ])\n",
        "\n",
        "  aug_img_arr = data_augmentation(img4d)\n",
        "\n",
        "  aug_img = Image.fromarray(aug_img_arr.numpy()[0].astype(np.uint8))\n",
        "    \n",
        "  return aug_img\n",
        "\n",
        "def add_train_datapoint_cassava(image, image_id, label, train_images_dir_path, train_csv_path):\n",
        "    datapoint = dict({\n",
        "        'image_id': image_id,\n",
        "        'label': label,\n",
        "    })\n",
        "    \n",
        "    if not os.path.exists(train_images_dir_path):\n",
        "        os.makedirs(train_images_dir_path)\n",
        "      \n",
        "    image.save(train_images_dir_path + str(image_id)) # save\n",
        "  \n",
        "    with open(train_csv_path, 'a') as f:\n",
        "      writer = csv.DictWriter(f, ['image_id', 'label'])\n",
        "      writer.writerow(datapoint)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oGOjP8bCQEn"
      },
      "source": [
        "def get_data_with_label(data, label):\n",
        "  return data.loc[data['label'] == label]\n",
        "\n",
        "def create_seperate_set(set_data, dir_name):\n",
        "  img_ids = set_data['image_id']\n",
        "  for id in img_ids:\n",
        "    os.rename(f\"/content/train_images/{id}\", f\"/content/{dir_name}/{id}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBXZ0g72GudX"
      },
      "source": [
        "!mkdir val_images\n",
        "!mkdir test_images\n",
        "\n",
        "random_state = np.random.RandomState(69420)\n",
        "\n",
        "train_data = pd.read_csv('train.csv')\n",
        "\n",
        "val_data = pd.DataFrame(data=None, columns=train_data.columns) \n",
        "test_data = pd.DataFrame(data=None, columns=train_data.columns) \n",
        "\n",
        "for i in range(5):\n",
        "  test_rows = train_data[train_data[\"label\"] == i].sample(n=200, random_state=random_state)\n",
        "  test_data = test_data.append(test_rows)\n",
        "  train_data = train_data.drop(test_rows.index)\n",
        "\n",
        "create_seperate_set(test_data, 'test_images')\n",
        "\n",
        "for i in range(5):\n",
        "  val_rows = train_data[train_data[\"label\"] == i].sample(n=200, random_state=random_state)\n",
        "  val_data = val_data.append(val_rows)\n",
        "  train_data = train_data.drop(val_rows.index)\n",
        "\n",
        "create_seperate_set(val_data, 'val_images')\n",
        "\n",
        "\n",
        "val_data.to_csv('val_data.csv')\n",
        "test_data.to_csv('test_data.csv')\n",
        "\n",
        "\n",
        "with open('augmented_data.csv', 'a') as f:\n",
        "      writer = csv.DictWriter(f, ['image_id', 'label'])\n",
        "      writer.writerow(dict({\n",
        "        'image_id': 'image_id',\n",
        "        'label': 'label',\n",
        "      }))\n",
        "\n",
        "unique_labels = set(test_data['label'])\n",
        "\n",
        "n_aug_for_balance = {}\n",
        "\n",
        "for label in unique_labels:\n",
        "  n_aug_for_balance[label] = len(train_data) - len(get_data_with_label(train_data, label))\n",
        "\n",
        "n_aug_for_balance_largest_class = min(n_aug_for_balance.values())\n",
        "\n",
        "for label in unique_labels:\n",
        "  n_aug_for_balance[label] -= n_aug_for_balance_largest_class\n",
        "\n",
        "for label in n_aug_for_balance.keys():\n",
        "  data_filtered_by_label = get_data_with_label(train_data, label)\n",
        "  for i in range(n_aug_for_balance[label]):\n",
        "    print(label, i)\n",
        "    datapoint_to_augment = data_filtered_by_label.iloc[i % len(data_filtered_by_label)]\n",
        "\n",
        "    image = cv2.imread('train_images/' + datapoint_to_augment['image_id'])\n",
        "    augmented_image = augment_image(image)\n",
        "    \n",
        "    add_train_datapoint_cassava(\n",
        "        image=augmented_image, \n",
        "        image_id='aug_'+str(label)+'_'+str(i)+'_'+datapoint_to_augment['image_id'], \n",
        "        label=datapoint_to_augment['label'],\n",
        "        train_images_dir_path='augmented_images/',\n",
        "        train_csv_path='augmented_data.csv'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18W2YTpoWeKh"
      },
      "source": [
        "for file_name in os.listdir('augmented_images/'):\n",
        "    shutil.move(os.path.join('augmented_images', file_name), 'train_images')\n",
        "\n",
        "augmented_data = pd.read_csv('augmented_data.csv')\n",
        "pd.concat([train_data, augmented_data], ignore_index=True).to_csv('train_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4KKcxJU_rUp"
      },
      "source": [
        "!zip -r train_images.zip train_images\n",
        "!zip -r val_images.zip val_images\n",
        "!zip -r test_images.zip test_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af-DR3oAHAA7"
      },
      "source": [
        "!mv /content/train_images.zip /content/drive/MyDrive/train_images.zip\n",
        "!mv /content/val_images.zip /content/drive/MyDrive/val_images.zip\n",
        "!mv /content/test_images.zip /content/drive/MyDrive/test_images.zip\n",
        "!mv /content/train_data.csv /content/drive/MyDrive/train_data.csv\n",
        "!mv /content/val_data.csv /content/drive/MyDrive/val_data.csv\n",
        "!mv /content/test_data.csv /content/drive/MyDrive/test_data.csv"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
